{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66304729",
   "metadata": {},
   "source": [
    "# Multivariate Analsysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e877000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befd97bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9315495e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "%matplotlib inline\n",
    "%matplotlib ipympl\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaf60e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(r\"C:\\work\\water_use\\ml_experiments\\annual_v_0_0\\clean_train_db.csv\")\n",
    "raw_dataset = pd.read_csv(r\"C:\\work\\water_use\\mldataset\\ml\\training\\train_datasets\\Annual\\raw_wu_annual_training.csv\")\n",
    "pop_info = pd.read_csv(r\"pop_info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e901ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfere pop from pop_info to dataset\n",
    "pop_info['pop'] = pop_info['pop_swud16'].copy()\n",
    "mask = (pop_info['pop'].isna()) | (pop_info['pop']==0)\n",
    "pop_info.loc[mask, 'pop'] = pop_info[mask]['plc_pop_interpolated']\n",
    "mask = (pop_info['pop'].isna()) | (pop_info['pop']==0)\n",
    "pop_info.loc[mask, 'pop'] = pop_info[mask]['TPOPSRV']\n",
    "mask = (pop_info['pop'].isna()) | (pop_info['pop']==0)\n",
    "pop_info.loc[mask, 'pop'] = pop_info[mask]['tract_pop']\n",
    "dataset = dataset[dataset['Ecode_num']==0]\n",
    "\n",
    "pop_df = pop_info[['sys_id', 'pop', 'Year']]\n",
    "dataset = dataset.merge(pop_df, right_on=['sys_id', 'Year'], left_on=['sys_id', 'Year'] , how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a2bc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer processed income and house ag from raw to master dataset\n",
    "cols_to_transfer = ['Year', 'sys_id']\n",
    "for col in raw_dataset.columns:\n",
    "    if  (\"h_age\" in col) \\\n",
    "    or (\"income\" in col) \\\n",
    "    or (\"median_h_year\" in col) \\\n",
    "    or (\"av_house_age\" in col) \\\n",
    "    or (\"n_houses\") in col:        \n",
    "        cols_to_transfer.append(col)\n",
    "        if col in dataset.columns:\n",
    "            del(dataset[col])\n",
    "    \n",
    "raw_dataset = raw_dataset[cols_to_transfer]       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a8b36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.merge(raw_dataset, right_on=['sys_id', 'Year'], left_on=['sys_id', 'Year'] , how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e32b6ef",
   "metadata": {},
   "source": [
    "Remove extreme population density values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c6e892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum pop density in the us is 22K/km2\n",
    "# I assume the minimum reasonable density of people to be 22 or 3% percintile)\n",
    "# https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population_density\n",
    "plt.figure()\n",
    "np.log10(dataset['pop']/dataset['WSA_SQKM']).hist(bins = 100)\n",
    "plt.show()\n",
    "dataset['pop_density'] = dataset['pop']/dataset['WSA_SQKM']\n",
    "len_before = len(dataset)\n",
    "dataset = dataset[dataset['pop_density']<= 22000]\n",
    "dataset = dataset[dataset['pop_density']>= 22]\n",
    "ratio = len(dataset)/len_before\n",
    "print(\"Percentage of rows with extrem pop density {}\".format(100*(1-ratio)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44790881",
   "metadata": {},
   "source": [
    "Remove extreme per capita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba9e2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_allowed_pc = 500\n",
    "min_allowed_pc = 20\n",
    "min_allowed_pop = 100\n",
    "\n",
    "\n",
    "train_db = dataset[dataset['wu_rate']>0]\n",
    "len_before = len(train_db)\n",
    "train_db['pc'] = train_db['wu_rate']/train_db['pop']\n",
    "train_db = train_db[train_db['pop']>min_allowed_pop]\n",
    "mask = (train_db['pc']>=min_allowed_pc) & (train_db['pc']<=max_allowed_pc)\n",
    "train_db = train_db[mask]\n",
    "ratio = 100 * (1-(len(train_db)/len_before))\n",
    "print(\"Percentage of extreme PC and Population < 100 is {}\".format(ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b19ede",
   "metadata": {},
   "source": [
    "## Effect of income on per-capita use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2618399e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d008c93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054eaf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_hyp_tests(var1 = 'average_income' , var2 = 'pc', step = 0.05):\n",
    "    \"\"\" We use income as a variable, but other variables can be used\"\"\"\n",
    "    # Partition the data  \n",
    "    quantiles = np.arange(0,1,step)\n",
    "    pc_sets = {}\n",
    "    pc_income = {}\n",
    "    ttest_results = []\n",
    "    for q in quantiles:\n",
    "        income0 = train_db[var1].quantile(q)\n",
    "        income1 = train_db[var1].quantile(q+step)\n",
    "        mask1 = train_db[var1]>= income0\n",
    "        mask2 = train_db[var1]<income1\n",
    "        key = q + step/2.0\n",
    "        pc_sets[key] =  train_db.loc[mask1 & mask2, var2].values\n",
    "        pc_income[key] = (income0 + income1)/2.0\n",
    "    set_keys = sorted(list(pc_sets.keys()))\n",
    "    hyp_test_results = []\n",
    "    for s1 in set_keys:\n",
    "        for s2 in set_keys:\n",
    "            if s2>s1:\n",
    "                ht = stats.ttest_ind(a=pc_sets[s1], b=pc_sets[s2], equal_var=False)\n",
    "                curr = [s1, s2, pc_income[s1], pc_income[s2],  ht.statistic, ht.pvalue]\n",
    "                hyp_test_results.append(curr)\n",
    "    columns = ['q1', 'q2', 'income1', 'income2', 'tstat', 'pvalue']\n",
    "    hyp_test_results = pd.DataFrame(hyp_test_results, columns=columns)    \n",
    "    del(pc_sets)\n",
    "    return hyp_test_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d430b558",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_test_results = multi_hyp_tests(var1 = 'average_income' , var2 = 'pc', step = 0.05)\n",
    "plt.figure()\n",
    "plt.scatter(hyp_test_results['q1'], hyp_test_results['q2'], c = np.log10(hyp_test_results['pvalue']), cmap = 'jet')\n",
    "plt.colorbar()\n",
    "reject = hyp_test_results[hyp_test_results['pvalue']>=0.05]\n",
    "plt.scatter(reject['q1'], reject['q2'], facecolors='none', s = 100, edgecolors='k')\n",
    "\n",
    "# plt.gca().set_yscale('log')\n",
    "# plt.gca().set_xscale('log')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f47a545",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf24b4cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1279102c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# #plt.scatter(train_db['average_income'], train_db['pc'], s=1, c = [0.7,0.7,0.7])\n",
    "# plt.plot(ttest_results['Income'], ttest_results['ttest_stat'])\n",
    "# secax = plt.gca().twinx()\n",
    "# secax.plot(ttest_results['Income'], np.log10(ttest_results['ttest_pvalue']), 'k' )\n",
    "\n",
    "# plt.figure()\n",
    "# #plt.plot(ttest_results['Income'], ttest_results['u_median'])\n",
    "# arr = stats.binned_statistic(train_db['average_income'],  train_db['pc'], statistic='median', bins=20)\n",
    "# #plt.scatter(train_db['average_income'], train_db['pc'])\n",
    "# plt.plot(arr.bin_edges[1:], arr.statistic)\n",
    "# #arr = stats.binned_statistic(train_db['average_income'],  train_db['pc'], statistic='mean', bins=10)\n",
    "\n",
    "# #plt.plot(arr.bin_edges[1:], arr.statistic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7379ccc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "slopes = []\n",
    "sys_db = train_db.groupby(by = 'sys_id').mean()\n",
    "for i in range(4000):\n",
    "    xy = train_db[['average_income', 'pc']]\n",
    "    n = np.random.randint(300)\n",
    "    a = xy.sample(n, random_state = np.random.randint(1000))\n",
    "    b = xy.sample(n, random_state = np.random.randint(1000))\n",
    "    sign = ((a['pc'].values-b['pc'].values)/(a['average_income'].values-b['average_income'].values))>0\n",
    "    ss = 100*np.sum(sign)/n\n",
    "    slopes.append(ss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f2f804",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "xy = plt.hist(slopes, bins = 50)\n",
    "plt.plot([50, 50], [0, np.max(xy[0])], label = \"50% Cutoff\")\n",
    "plt.xlabel(\"Fraction of Positive slopes ($\\delta PerCapita/\\delta Income$)\")\n",
    "plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee7cfa3",
   "metadata": {},
   "source": [
    "### Use non-parametric tests\n",
    "* Spearman's rank correlation coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc621311",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "xy = train_db[train_db['average_income']<train_db['average_income'].quantile(0.99)]\n",
    "arr = stats.binned_statistic(xy['average_income'],  xy['pc'], statistic='mean', bins=20)\n",
    "#coef, p = spearmanr(train_db['average_income'], train_db['pc'])\n",
    "coef, p = spearmanr(arr.bin_edges[1:], arr.statistic)\n",
    "print('Spearmans correlation coefficient: %.3f' % coef)\n",
    "# interpret the significance\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "\tprint('Samples are uncorrelated (fail to reject H0) p=%.3f' % p)\n",
    "else:\n",
    "\tprint('Samples are correlated (reject H0) p=%.3f' % p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372fb673",
   "metadata": {},
   "source": [
    "* Kendall rank correlation coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153cb040",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kendalltau\n",
    "# calculate kendall's correlation\n",
    "xy = train_db[train_db['average_income']<train_db['average_income'].quantile(0.99)]\n",
    "arr = stats.binned_statistic(xy['average_income'],  xy['pc'], statistic='mean', bins=100)\n",
    "#coef, p = kendalltau(xy['average_income'], xy['pc'])\n",
    "coef, p = kendalltau(arr.bin_edges[1:], arr.statistic)\n",
    "print('Kendall correlation coefficient: %.3f' % coef)\n",
    "# interpret the significance\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "\tprint('Samples are uncorrelated (fail to reject H0) p=%.3f' % p)\n",
    "else:\n",
    "\tprint('Samples are correlated (reject H0) p=%.3f' % p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcc237c",
   "metadata": {},
   "source": [
    "### Scatter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d522fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter(varX = 'average_income', varY = 'pc', xlimits = [30000,120000], \n",
    "                ylimits = [20,300], xlabel = \"Average Income in Service Area\",\n",
    "                ylabel = \"Gallon Per Capita per day Water Use\", xlog = False,\n",
    "                ylog = False):\n",
    "    left, width = 0.1, 0.65\n",
    "    bottom, height = 0.1, 0.65\n",
    "    spacing = 0.005\n",
    "\n",
    "    x = train_db[varX]\n",
    "    y = train_db[varY]\n",
    "    arr = stats.binned_statistic(x,  y, statistic='mean', bins=50)\n",
    "    pc_limits = np.array(ylimits)\n",
    "    income_limits = np.array(xlimits)\n",
    "    rect_scatter = [left, bottom, width, height]\n",
    "    rect_histx = [left, bottom + height + spacing, width, 0.2]\n",
    "    rect_histy = [left + width + spacing, bottom, 0.2, height]\n",
    "\n",
    "    # start with a rectangular Figure\n",
    "    plt.figure(figsize=(8, 8))\n",
    "\n",
    "    ax_scatter = plt.axes(rect_scatter)\n",
    "    ax_scatter.tick_params(direction='in', top=True, right=True)\n",
    "    ax_histx = plt.axes(rect_histx)\n",
    "    ax_histx.tick_params(direction='in', labelbottom=False)\n",
    "    ax_histy = plt.axes(rect_histy)\n",
    "    ax_histy.tick_params(direction='in', labelleft=False)\n",
    "\n",
    "    # the scatter plot:\n",
    "    ax_scatter.scatter(x, y, s = 1, c = [0.7,0.7,0.7])\n",
    "    ax_scatter.plot(arr.bin_edges[1:], arr.statistic)\n",
    "    ax_scatter.set_ylim((pc_limits[0], pc_limits[1]))\n",
    "    ax_scatter.set_xlim((income_limits[0], income_limits[1]))\n",
    "    ax_scatter.set_xlabel(xlabel)\n",
    "    ax_scatter.set_ylabel(ylabel)\n",
    "    \n",
    "\n",
    "    # bins = np.arange(-lim, lim + binwidth, binwidth)\n",
    "    ax_histx.hist(x, bins=100)\n",
    "    ax_histy.hist(y, bins=100, orientation='horizontal')\n",
    "\n",
    "    ax_histx.set_xlim(ax_scatter.get_xlim())\n",
    "    ax_histy.set_ylim(ax_scatter.get_ylim())\n",
    "    \n",
    "    if xlog:\n",
    "        ax_scatter.set_xscale('log')\n",
    "        ax_histx.set_xscale('log')\n",
    "    if ylog:\n",
    "        ax_scatter.set_yscale('log')\n",
    "        ax_histx.set_yscale('log')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9f6af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter(varX = 'average_income', varY = 'pc', xlimits = [30000,120000], \n",
    "                ylimits = [20,300], xlabel = \"Average Income in Service Area\",\n",
    "                ylabel = \"Gallon Per Capita per day Water Use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530d3556",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0968cb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lot size + income?\n",
    "# cost of living + income?\n",
    "# median p25\n",
    "# https://www.aceee.org/research-report/u2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a77db3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b58187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3bd4e7fd",
   "metadata": {},
   "source": [
    "## How population density affects per-capita-water use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa23738",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Effect of population density\n",
    "# 1) split the data based on a certain income into two sets\n",
    "# 2) test if the mean/median of the two population are significantly different\n",
    "hyp_test_results = multi_hyp_tests(var1 = 'pop_density' , var2 = 'pc', step = 0.05)\n",
    "plt.figure()\n",
    "plt.scatter(hyp_test_results['q1'], hyp_test_results['q2'], c = np.log10(hyp_test_results['pvalue']), cmap = 'jet')\n",
    "plt.colorbar()\n",
    "reject = hyp_test_results[hyp_test_results['pvalue']>=0.05]\n",
    "plt.scatter(reject['q1'], reject['q2'], facecolors='none', s = 100, edgecolors='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6497c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_db['log_pop_density'] = np.log10(train_db['pop_density'])\n",
    "plot_scatter(varX = 'log_pop_density', varY = 'pc', xlimits = [1,4], \n",
    "                ylimits = [20,300], xlabel = \"Log10 of Density\",\n",
    "                ylabel = \"Gallon Per Capita per day Water Use\")\n",
    "del(train_db['log_pop_density'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93301b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c990d5d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3652c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = train_db[['pop_density', 'pc']]\n",
    "n = 100\n",
    "slopes = []\n",
    "for i in range(4000):\n",
    "    n = np.random.randint(300)\n",
    "    a = xy.sample(n, random_state = np.random.randint(1000))\n",
    "    b = xy.sample(n, random_state = np.random.randint(1000))\n",
    "    sign = ((a['pc'].values-b['pc'].values)/(a['pop_density'].values-b['pop_density'].values))>0\n",
    "    ss = np.sum(sign)/n\n",
    "    slopes.append(ss)\n",
    "plt.figure()\n",
    "xy = plt.hist(slopes, bins = 100)\n",
    "plt.plot([0.5, 0.5], [0, np.max(xy[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40816720",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7ae023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Spearman_corr(feature = 'pop_density', target = 'pc', feature_trim = 0.99, bins = 20):\n",
    "    from scipy.stats import spearmanr\n",
    "    \n",
    "    xy = train_db[train_db[feature]<train_db[feature].quantile(feature_trim)]\n",
    "    xy = xy.dropna(axis=0)\n",
    "    arr = stats.binned_statistic(xy[feature],  xy[target], statistic='mean', bins=bins)\n",
    "    xval = arr.bin_edges[1:]\n",
    "    yval = arr.statistic\n",
    "    mask = np.logical_not(np.isnan(yval))\n",
    "    xval= xval[mask]\n",
    "    yval = yval[mask]\n",
    "    #coef, p = spearmanr(train_db['average_income'], train_db['pc'])\n",
    "    coef, p = spearmanr(xval, yval)\n",
    "    print('Spearmans correlation coefficient: %.3f' % coef)\n",
    "    # interpret the significance\n",
    "    alpha = 0.05\n",
    "    if p > alpha:\n",
    "        print('Samples are uncorrelated (fail to reject H0) p=%.3f' % p)\n",
    "    else:\n",
    "        print('Samples are correlated (reject H0) p=%.3f' % p)\n",
    "    return coef, p\n",
    "\n",
    "def kendalltau_corr(feature = 'pop_density', target = 'pc', feature_trim = 0.99, bins = 20):\n",
    "    from scipy.stats import kendalltau   \n",
    "    xy = train_db[train_db[feature]<train_db[feature].quantile(feature_trim)]\n",
    "    xy = xy.dropna(axis=0)\n",
    "    arr = stats.binned_statistic(xy[feature],  xy[target], statistic='mean', bins=bins)\n",
    "    xval = arr.bin_edges[1:]\n",
    "    yval = arr.statistic\n",
    "    mask = np.logical_not(np.isnan(yval))\n",
    "    xval= xval[mask]\n",
    "    yval = yval[mask]\n",
    "    #coef, p = spearmanr(train_db['average_income'], train_db['pc'])\n",
    "    coef, p = kendalltau(xval, yval)\n",
    "    print('Kendall correlation coefficient: %.3f' % coef)\n",
    "    # interpret the significance\n",
    "    alpha = 0.05\n",
    "    if p > alpha:\n",
    "        print('Samples are uncorrelated (fail to reject H0) p=%.3f' % p)\n",
    "    else:\n",
    "        print('Samples are correlated (reject H0) p=%.3f' % p)\n",
    "    return coef, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1275fefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Spearman_corr(feature = 'pop_density', target = 'pc', feature_trim = 0.99, bins = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939512a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "kendalltau_corr(feature = 'pop_density', target = 'pc', feature_trim = 0.99, bins = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3aa471",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_list = ['population', 'households2', 'sys_id', 'wu_rate', 'HUC2', 'county_id', 'Ecode_num', 'pop_house_ratio', 'state_id',\n",
    "            'KG_climate_zone', 'tot_h_age', 'pc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4a466c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rank_tests = []\n",
    "for col in train_db.columns:\n",
    "    if col in skip_list:\n",
    "        continue\n",
    "    coef0, p0 = Spearman_corr(feature = col, target = 'pc', feature_trim = 0.99, bins = 50)\n",
    "    coef1, p1 = kendalltau_corr(feature = col, target = 'pc', feature_trim = 0.99, bins = 50)\n",
    "    all_rank_tests.append([col, coef0, p0, coef1, p1])\n",
    "    \n",
    "cols = ['Feature', 'Spearman_corr', 'Spear_pval', 'kendall_corr', 'kendall_pval']\n",
    "all_rank_tests = pd.DataFrame(all_rank_tests, columns=cols);\n",
    "all_rank_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afa7910",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rank_tests.iloc[50:120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e70dbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rank_tests['abs'] = np.abs(all_rank_tests['Spearman_corr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bc0007",
   "metadata": {},
   "source": [
    "(income-ztranform/lot_size) or (income * lot_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a4994c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = all_rank_tests.sort_values(by = ['abs'], ascending = False)\n",
    "cc.iloc[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c01579",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc.to_csv(r\"stats.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb1bf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig.ax_joint.plot()\n",
    "arr = stats.binned_statistic(train_db['av_house_age'],  train_db['pc'], statistic='mean', bins=50)\n",
    "fig.ax_joint.plot(arr.bin_edges[1:], arr.statistic, color = 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86e6b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = train_db[['av_house_age', 'pc']]\n",
    "n = 100\n",
    "slopes = []\n",
    "for i in range(4000):\n",
    "    n = np.random.randint(300)\n",
    "    a = xy.sample(n, random_state = np.random.randint(1000))\n",
    "    b = xy.sample(n, random_state = np.random.randint(1000))\n",
    "    sign = ((a['pc'].values-b['pc'].values)/(a['av_house_age'].values-b['av_house_age'].values))>0\n",
    "    ss = np.sum(sign)/n\n",
    "    slopes.append(ss)\n",
    "plt.figure()\n",
    "xy = plt.hist(slopes, bins = 100)\n",
    "plt.plot([0.5, 0.5], [0, np.max(xy[0])])\n",
    "slopes = np.array(slopes)\n",
    "slopes = slopes[~np.isnan(slopes)]\n",
    "one_sample = stats.ttest_1samp(slopes, 0.5)\n",
    "\n",
    "print (\"The t-statistic is %.3f and the p-value is %.3f.\" % one_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d6fdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "xy = train_db[train_db['av_house_age']<train_db['av_house_age'].quantile(0.99)]\n",
    "arr = stats.binned_statistic(xy['av_house_age'],  xy['pc'], statistic='mean', bins=20)\n",
    "#coef, p = spearmanr(train_db['average_income'], train_db['pc'])\n",
    "coef, p = spearmanr(arr.bin_edges[1:], arr.statistic)\n",
    "print('Spearmans correlation coefficient: %.3f' % coef)\n",
    "# interpret the significance\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "\tprint('Samples are uncorrelated (fail to reject H0) p=%.3f' % p)\n",
    "else:\n",
    "\tprint('Samples are correlated (reject H0) p=%.3f' % p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8923dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b17975",
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = train_db[['pop', 'pc']]\n",
    "n = 100\n",
    "slopes = []\n",
    "for i in range(4000):\n",
    "    n = np.random.randint(100)\n",
    "    a = xy.sample(n, random_state = np.random.randint(1000))\n",
    "    xy = xy.sample(frac=1).reset_index(drop=True)\n",
    "    b = xy.sample(n, random_state = np.random.randint(1000))\n",
    "    sign = ((a['pc'].values-b['pc'].values)/(a['pop'].values-b['pop'].values))>=0\n",
    "    ss = np.sum(sign)/n\n",
    "    slopes.append(ss)\n",
    "plt.figure()\n",
    "xy = plt.hist(slopes, bins = 100)\n",
    "plt.plot([0.5, 0.5], [0, np.max(xy[0])])\n",
    "slopes = np.array(slopes)\n",
    "slopes = slopes[~np.isnan(slopes)]\n",
    "one_sample = stats.ttest_1samp(slopes, 0.5)\n",
    "\n",
    "print (\"The t-statistic is %.3f and the p-value is %.3f.\" % one_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38b5333",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d474cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kendalltau\n",
    "# calculate kendall's correlation\n",
    "xy = train_db[train_db['pop']<train_db['pop'].quantile(0.99)]\n",
    "arr = stats.binned_statistic(xy['pop'],  xy['pc'], statistic='mean', bins=100)\n",
    "#coef, p = kendalltau(xy['average_income'], xy['pc'])\n",
    "coef, p = kendalltau(arr.bin_edges[1:], arr.statistic)\n",
    "print('Kendall correlation coefficient: %.3f' % coef)\n",
    "# interpret the significance\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "\tprint('Samples are uncorrelated (fail to reject H0) p=%.3f' % p)\n",
    "else:\n",
    "\tprint('Samples are correlated (reject H0) p=%.3f' % p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77912c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "xy = train_db[train_db['pop']<train_db['pop'].quantile(0.99)]\n",
    "arr = stats.binned_statistic(xy['pop'],  xy['pc'], statistic='mean', bins=20)\n",
    "#coef, p = spearmanr(train_db['average_income'], train_db['pc'])\n",
    "coef, p = spearmanr(arr.bin_edges[1:], arr.statistic)\n",
    "print('Spearmans correlation coefficient: %.3f' % coef)\n",
    "# interpret the significance\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "\tprint('Samples are uncorrelated (fail to reject H0) p=%.3f' % p)\n",
    "else:\n",
    "\tprint('Samples are correlated (reject H0) p=%.3f' % p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c295e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig= sns.jointplot(np.log10(train_db['pop']), y=train_db['pc'], kind=\"hist\")\n",
    "arr = stats.binned_statistic(np.log10(train_db['pop']),  train_db['pc'], statistic='mean', bins=50)\n",
    "fig.ax_joint.plot(arr.bin_edges[1:], arr.statistic, color = 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea04f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(10000)\n",
    "y = np.random.rand(10000)\n",
    "xy = pd.DataFrame({'x':x, 'y':y})\n",
    "n = 100\n",
    "slopes = []\n",
    "for i in range(4000):\n",
    "    n = np.random.randint(2000)\n",
    "    a = xy.sample(n, random_state = np.random.randint(1000))\n",
    "    b = xy.sample(n, random_state = np.random.randint(1000))\n",
    "    sign = ((a['x'].values-b['x'].values)/(a['y'].values-b['y'].values))>0\n",
    "    ss = np.sum(sign)/n\n",
    "    slopes.append(ss)\n",
    "plt.figure()\n",
    "xy = plt.hist(slopes, bins = 100)\n",
    "plt.plot([0.5, 0.5], [0, np.max(xy[0])])\n",
    "slopes = np.array(slopes)\n",
    "slopes = slopes[~np.isnan(slopes)]\n",
    "one_sample = stats.ttest_1samp(slopes, 0.5)\n",
    "\n",
    "print (\"The t-statistic is %.3f and the p-value is %.3f.\" % one_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4598f78c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
